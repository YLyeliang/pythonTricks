{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## k近邻模型\n",
    "\n",
    "### 算法\n",
    "\n",
    "输入: $T=\\{(x_1,y_1),(x_2,y_2),\\dots,(x_N,y_N)\\}， x_i\\in \\cal{X} \\sube{\\bf{R}^n}, y_i\\in\\cal{Y}=\\{c_1,c_2,\\dots, c_k\\}$; 实例特征向量$x$\n",
    "\n",
    "输出: 实例所属的$y$\n",
    "\n",
    "步骤:\n",
    "\n",
    "1. 根据指定的**距离度量**，在$T$中查找$x$的**最近邻的k个点**，覆盖这$k$个点的$x$的邻域定义为$N_k(x)$\n",
    "\n",
    "1. 在$N_k(x)$中应用 **分类决策规则** 决定$x$的类别$y$,\n",
    "$\n",
    "y=\\arg\\max_{c_j}\\sum_{x_i\\in N_k(x)}I(y_i=c_j), i=1,2,\\dots,N, j=1,2,\\dots,K\n",
    "$. 该公式就是多数投票规则；I(x)即0-1函数\n",
    "\n",
    "这里提到了$k$近邻模型的三要素，如算法描述中黑体标注的部分， 注意这里的三要素和前面说的统计学习方法的三要素不是一个东西。后面讲到[隐马尔可夫模型](CH10/README.md)的时候也有三要素。\n",
    "\n",
    "“多数表决”分类会在类别分布偏斜时出现缺陷。解决这个缺点的方法之一是在进行分类时将样本到k个近邻点的距离考虑进去。\n",
    "k近邻点中每一个的分类（对于回归问题来说，是数值）都乘以与测试点之间距离的成反比的权重。\n",
    "\n",
    "### 距离度量\n",
    "\n",
    "> **特征空间**中的两个实例点的距离是两个实例点相似程度的反映。\n",
    "\n",
    "书中是如上描述的，这里要注意**距离越近(数值越小)， 相似度越大**。\n",
    "\n",
    "\n",
    "\n",
    "这里用到了$L_p$距离, 可以参考Wikipedia上$L_p$ Space词条[^1]\n",
    "\n",
    "1. $p=1$ 对应 曼哈顿距离\n",
    "1. $p=2$ 对应 欧氏距离\n",
    "1. 任意$p$ 对应 闵可夫斯基距离\n",
    "\n",
    "\n",
    "$$L_p(x_i, x_j)=\\left(\\sum_{l=1}^{n}{\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^p}\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "KD-Tree\n",
    "\n",
    "[kD-Tree](https://zhuanlan.zhihu.com/p/45346117)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# finding the nearest neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# 对于简单的两个数据集的最近邻距离，以k=2为例\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "print(indices)\n",
    "print(distances)\n",
    "nbrs.kneighbors_graph(X).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [3 4]\n",
      " [4 3]\n",
      " [5 4]]\n",
      "[[0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.41421356]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.41421356]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[1., 1., 0., 0., 0., 0.],\n       [1., 1., 0., 0., 0., 0.],\n       [0., 1., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 1., 0.],\n       [0., 0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 1., 1.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1],\n       [1, 0],\n       [2, 1],\n       [3, 4],\n       [4, 3],\n       [5, 4]], dtype=int64)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KDTree and BallTree Classes\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kdt = KDTree(X, leaf_size=30, metric='euclidean')\n",
    "kdt.query(X, k=2, return_distance=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "kNN分类和回归\n",
    "\n",
    "分类一般采用多数投票的方式，回归则一般采用k个邻域内的点的实值取均值；\n",
    "\n",
    "分类：scikit中有两种；基于k近邻和基于半径radius;后者则对指定半径内的多个近邻点进行计算；"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}